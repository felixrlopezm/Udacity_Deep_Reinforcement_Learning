{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### Preliminary actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: introduce in the following cell, in `file_name`, the path to the file of the REACHER evironment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Launching the environment\n",
    "#env = UnityEnvironment(file_name='Reacher.app')      # single-agent version\n",
    "env = UnityEnvironment(file_name='Reacher-20.app')    # 20-agent version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity ML-Agents environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available brains: 1\n",
      "Name of the brains: ['ReacherBrain']\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Getting and selecting the default brain\n",
    "brain_names = env.brain_names\n",
    "print('Number of available brains:',len(brain_names))\n",
    "print('Name of the brains:',brain_names)\n",
    "\n",
    "brain_name = brain_names[0]          # Getting the name of the first brain\n",
    "brain = env.brains[brain_name]       # Initializing the brain\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Action space size: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# Reseting the environment\n",
    "env_data = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents in the environment\n",
    "num_agents = len(env_data.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# Action space size\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Action space size:', action_size)\n",
    "\n",
    "# State space size \n",
    "states = env_data.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent with DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters definition\n",
    "hyperparameters = {\n",
    "    'FC1': 128,               # Actor/Critic network: nodes at 1st hidden layer\n",
    "    'FC2': 128,               # Actor/Critic network: nodes at 2nd hidden layer\n",
    "    'BUFFER_SIZE': int(1e5),  # Replay buffer size\n",
    "    'BATCH_SIZE': 128,        # Minibatch size\n",
    "    'GAMMA': 0.93,            # Discount factor\n",
    "    'TAU': 1e-3,              # For soft update of target parameters\n",
    "    'LR_ACTOR': 5e-4,         # Learning rate of the actor\n",
    "    'LR_CRITIC': 1e-4,        # Learning rate of the critic\n",
    "    'WEIGHT_DECAY': 0,        # Critic network L2 weight decay\n",
    "    'UPDATE_EVERY': 20,       # Update rate\n",
    "    'N_UPDATES': 10,          # Update passes\n",
    "    'MU': 0.0,                # Parameter for Ornstein-Uhlenbeck noise\n",
    "    'SIGMA': 0.05,            # Parameter for Ornstein-Uhlenbeck noise\n",
    "    'THETA': 0.10,            # Parameter for Ornstein-Uhlenbeck noise\n",
    "    'RANDOM_SEED': 4          # Seed for random generation (to allow repetitiveness)\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the DDPG agent (from ddpg_agent.py file)\n",
    "agent = Agent(state_size=state_size, action_size=action_size, hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training process\n",
    "def ddpg_training(n_episodes=200, print_every=10):\n",
    "    scores_window = deque(maxlen=100)               # Initialize list of last 100 scores\n",
    "    scores = []                                     # Initialize list containing score for each episode\n",
    "    not_solved = True                               # initialize not_solved contro\n",
    "    solved_episode = 0                              # Initialize episode for average score over threshold\n",
    "    solved_av_score = 0                             # Initialize average score when solved\n",
    "    max_av_score = 0                                # Initialize maximum average score\n",
    "    max_av_score_episode = 0                        # Initialize episode at maximum average score\n",
    "\n",
    "    \n",
    "    # Episode loop\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_data = env.reset(train_mode=True)[brain_name]    # Reset environment in TRAINING MODE\n",
    "        states = env_data.vector_observations                # Get the first state\n",
    "        agent.reset()                                        # Reset smart agent (noise)\n",
    "        \n",
    "        score = np.zeros(num_agents)                         # Initialize score counter\n",
    "        t = 0                                                # Initialize time step counter\n",
    "        \n",
    "        # Trajectory loop\n",
    "        while True:\n",
    "            actions = agent.act(states)                      # Get action from policy (one per agent)\n",
    "            \n",
    "            env_data = env.step(actions)[brain_name]         # Interaction with the environment\n",
    "            next_states = env_data.vector_observations       # Get the next state (one per agent)\n",
    "            rewards = env_data.rewards                       # Get the reward (one per agent)\n",
    "            dones = env_data.local_done                      # Get the done code (one per agent)\n",
    "\n",
    "            agent.step20(states, actions, rewards, next_states, dones, t)   # Agent's process: gather experiences and learn\n",
    "            \n",
    "            states = next_states                 # Roll over states for next iteration\n",
    "            score += rewards                     # Add time step reward to total trajectory score\n",
    "            t += 1                               # +a at time step counter\n",
    "            \n",
    "            if np.any(dones):                    # Break trajectory loop if episode finishes (done=True)\n",
    "                break\n",
    "            \n",
    "\n",
    "        scores_window.append(np.mean(score))     # Append 20-agent mean total score of the trajectory to last 100 scores list\n",
    "        scores.append(np.mean(score))            # Append 20-agent mean total score of the trajectory to scores list\n",
    "        \n",
    "        # Printing training log\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(score)), end='')\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(score)))\n",
    "            \n",
    "        if (np.mean(scores_window) >= 30.0) and not_solved:\n",
    "            solved_episode = i_episode\n",
    "            solved_av_score = np.mean(scores_window)\n",
    "            not_solved = False\n",
    "            \n",
    "        # Saving agent and critic checkpoint files\n",
    "        if np.mean(scores_window) >= max_av_score:\n",
    "            max_av_score = np.mean(scores_window)\n",
    "            max_av_score_episode = i_episode\n",
    "            torch.save(agent.actor_local.state_dict(), 'trained_actor-ddpg.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'trained_critic-ddpg.pth')\n",
    "            \n",
    "    print('\\n')        \n",
    "    print('Problem solved in {:d} episodes with an average score of {:.2f}'.format(solved_episode, solved_av_score))\n",
    "    print('Maximum average score {:.2f}, reached at episode {:d}'.format(max_av_score, max_av_score_episode))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.85\tScore: 0.91\n",
      "Episode 20\tAverage Score: 1.07\tScore: 1.51\n",
      "Episode 30\tAverage Score: 1.46\tScore: 2.39\n",
      "Episode 40\tAverage Score: 1.89\tScore: 3.43\n",
      "Episode 50\tAverage Score: 2.43\tScore: 6.05\n",
      "Episode 60\tAverage Score: 3.42\tScore: 11.24\n",
      "Episode 70\tAverage Score: 5.07\tScore: 18.91\n",
      "Episode 80\tAverage Score: 7.87\tScore: 31.76\n",
      "Episode 90\tAverage Score: 10.94\tScore: 35.90\n",
      "Episode 100\tAverage Score: 13.60\tScore: 37.44\n",
      "Episode 110\tAverage Score: 17.26\tScore: 37.77\n",
      "Episode 120\tAverage Score: 21.01\tScore: 38.13\n",
      "Episode 130\tAverage Score: 24.62\tScore: 36.97\n",
      "Episode 140\tAverage Score: 28.15\tScore: 37.76\n",
      "Episode 150\tAverage Score: 31.52\tScore: 37.08\n",
      "\n",
      "\n",
      "Problem solved in 146 episodes with an average score of 30.20\n",
      "Maximum average score 31.52, reached at episode 150\n"
     ]
    }
   ],
   "source": [
    "# Lauching the training process\n",
    "scores = ddpg_training(n_episodes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting the scores evolution during training.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actor and critic weights from file (if needed)\n",
    "agent.actor_local.load_state_dict(torch.load('trained_actor-ddpg.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('trained_critic-ddpg.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Agent Score (averaged over all 20 agents): 37.60\n"
     ]
    }
   ],
   "source": [
    "# Checking the performance of the trained agent\n",
    "env_data = env.reset(train_mode=False)[brain_name]     # Reset environment\n",
    "states = env_data.vector_observations                  # Get the first state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # Initialize the score (for each agent)\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states)                        # Get action from policy (one per agent)\n",
    "    env_data = env.step(actions)[brain_name]           # Interaction with the environment\n",
    "    next_states = env_data.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_data.rewards                         # get reward (for each agent)\n",
    "    dones = env_data.local_done                        # See if episode finished\n",
    "    scores += env_data.rewards                         # Add the reward to the total score (for each agent)\n",
    "    states = next_states                               # Roll over states to next time step\n",
    "    if np.any(dones):                                  # Exit loop if episode finished\n",
    "        break\n",
    "        \n",
    "print('Smart Agent Score (averaged over all 20 agents): {:.2f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Agent Score (averaged over all 20 agents): 0.23\n"
     ]
    }
   ],
   "source": [
    "# Taking random actions in the Environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # Reset the environment    \n",
    "states = env_info.vector_observations                  # Get the first state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # Initialize the score (for each agent)\n",
    "\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # eslect a random action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # Clip all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # Interaction with the environment\n",
    "    next_states = env_info.vector_observations         # Get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # Get reward (for each agent)\n",
    "    dones = env_info.local_done                        # See if episode finished\n",
    "    scores += env_info.rewards                         # Update the score (for each agent)\n",
    "    states = next_states                               # Roll over states to next time step\n",
    "    if np.any(dones):                                  # Exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print(\"Random Agent Score (averaged over all 20 agents): {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
